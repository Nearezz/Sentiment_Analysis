{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15539e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR = Path(\"tweet\") \n",
    "FILE_EXT = \".txt\"\n",
    "RAW_DIR = BASE_DIR / \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "486eb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/ddjwhrn11vn435ndqk2yyjnc0000gn/T/ipykernel_19558/137509198.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tweets_raw = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((119844, 319),\n",
       "                        created_at                  id              id_str  \\\n",
       " 0  Fri Jan 03 11:01:06 +0000 2014  419060848288464897  419060848288464897   \n",
       " 1  Thu Jan 02 23:38:26 +0000 2014  418889049609625600  418889049609625600   \n",
       " 2  Sun Jan 05 20:12:05 +0000 2014  419924283498831872  419924283498831872   \n",
       " 3  Thu Jan 09 05:17:01 +0000 2014  421148584059604992  421148584059604992   \n",
       " 4  Fri Jan 10 00:14:17 +0000 2014  421434786599546880  421434786599546880   \n",
       " \n",
       "                                                 text  \\\n",
       " 0  Weekly Dow #Stocks Trend $DIS $WMT $HD $GS $V ...   \n",
       " 1  $VZ - A New Year means time for new Dogs of th...   \n",
       " 2  $VZ The S&amp;Ps Worst Sectors in 2013 http://...   \n",
       " 3  $VZ - Why T-Mobile Bought Verizons Spectrum -&...   \n",
       " 4  #VIDEO #AccumulationDistribution in #EXCEL htt...   \n",
       " \n",
       "                                               source  truncated  \\\n",
       " 0  <a href=\"http://12stocks.com\" rel=\"nofollow\">d...      False   \n",
       " 1  <a href=\"http://yahoo.com\" rel=\"nofollow\">snnt...      False   \n",
       " 2  <a href=\"http://dlvr.it\" rel=\"nofollow\">dlvr.i...      False   \n",
       " 3  <a href=\"http://pagemy.de\" rel=\"nofollow\">time...      False   \n",
       " 4  <a href=\"http://itunes.apple.com/us/app/twitte...      False   \n",
       " \n",
       "    in_reply_to_status_id in_reply_to_status_id_str  in_reply_to_user_id  \\\n",
       " 0                    NaN                      None                  NaN   \n",
       " 1                    NaN                      None                  NaN   \n",
       " 2                    NaN                      None                  NaN   \n",
       " 3                    NaN                      None                  NaN   \n",
       " 4                    NaN                      None                  NaN   \n",
       " \n",
       "   in_reply_to_user_id_str  ... quoted_status.quoted_status_id  \\\n",
       " 0                    None  ...                            NaN   \n",
       " 1                    None  ...                            NaN   \n",
       " 2                    None  ...                            NaN   \n",
       " 3                    None  ...                            NaN   \n",
       " 4                    None  ...                            NaN   \n",
       " \n",
       "   quoted_status.quoted_status_id_str quoted_status.scopes.followers  \\\n",
       " 0                                NaN                            NaN   \n",
       " 1                                NaN                            NaN   \n",
       " 2                                NaN                            NaN   \n",
       " 3                                NaN                            NaN   \n",
       " 4                                NaN                            NaN   \n",
       " \n",
       "   retweeted_status.scopes.followers  \\\n",
       " 0                               NaN   \n",
       " 1                               NaN   \n",
       " 2                               NaN   \n",
       " 3                               NaN   \n",
       " 4                               NaN   \n",
       " \n",
       "   retweeted_status.quoted_status.quoted_status_id  \\\n",
       " 0                                             NaN   \n",
       " 1                                             NaN   \n",
       " 2                                             NaN   \n",
       " 3                                             NaN   \n",
       " 4                                             NaN   \n",
       " \n",
       "    retweeted_status.quoted_status.quoted_status_id_str  \\\n",
       " 0                                                NaN     \n",
       " 1                                                NaN     \n",
       " 2                                                NaN     \n",
       " 3                                                NaN     \n",
       " 4                                                NaN     \n",
       " \n",
       "    quoted_status.geo.type  quoted_status.geo.coordinates  \\\n",
       " 0                     NaN                            NaN   \n",
       " 1                     NaN                            NaN   \n",
       " 2                     NaN                            NaN   \n",
       " 3                     NaN                            NaN   \n",
       " 4                     NaN                            NaN   \n",
       " \n",
       "    quoted_status.coordinates.type quoted_status.coordinates.coordinates  \n",
       " 0                             NaN                                   NaN  \n",
       " 1                             NaN                                   NaN  \n",
       " 2                             NaN                                   NaN  \n",
       " 3                             NaN                                   NaN  \n",
       " 4                             NaN                                   NaN  \n",
       " \n",
       " [5 rows x 319 columns])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for company_dir in RAW_DIR.iterdir():\n",
    "    if not company_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    ticker = company_dir.name.upper()\n",
    "\n",
    "    # Load ALL files inside the company folder\n",
    "    files = sorted(f for f in company_dir.iterdir() if f.is_file())\n",
    "\n",
    "    for f in files:\n",
    "        records = []\n",
    "        with f.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in fh:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    records.append(obj)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        if not records:\n",
    "            continue\n",
    "\n",
    "        df = pd.json_normalize(records, sep=\".\")\n",
    "        df[\"folder_ticker\"] = ticker\n",
    "        df[\"source_file\"] = f.name\n",
    "        all_dfs.append(df)\n",
    "\n",
    "tweets_raw = pd.concat(all_dfs, ignore_index=True)\n",
    "tweets_raw.columns = tweets_raw.columns.str.lower().str.strip()\n",
    "\n",
    "tweets_raw.shape, tweets_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a649716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/ddjwhrn11vn435ndqk2yyjnc0000gn/T/ipykernel_19558/4282610652.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tweets_raw[\"symbols\"] = tweets_raw[\"entities.symbols\"].apply(extract_symbols)\n"
     ]
    }
   ],
   "source": [
    "def extract_symbols(symbol_list):\n",
    "    if not isinstance(symbol_list, list):\n",
    "        return []\n",
    "    out = []\n",
    "    for s in symbol_list:\n",
    "        t = s.get(\"text\", \"\").upper()\n",
    "        if t:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "tweets_raw[\"symbols\"] = tweets_raw[\"entities.symbols\"].apply(extract_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca79c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398104, 320)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_long = tweets_raw.explode(\"symbols\").rename(columns={\"symbols\": \"ticker\"})\n",
    "tweets_long = tweets_long[~tweets_long[\"ticker\"].isna()]\n",
    "tweets_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e16b22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_base_text(row):\n",
    "    rt = row.get(\"retweeted_status.text\")\n",
    "    if isinstance(rt, str) and rt.strip():\n",
    "        return rt\n",
    "    return row.get(\"text\", \"\")\n",
    "\n",
    "tweets_long[\"base_text\"] = tweets_long.apply(choose_base_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa6713ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/ddjwhrn11vn435ndqk2yyjnc0000gn/T/ipykernel_19558/4282747711.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  tweets_long[\"created_at\"] = pd.to_datetime(tweets_long[\"created_at\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "tweets_long[\"created_at\"] = pd.to_datetime(tweets_long[\"created_at\"], errors=\"coerce\")\n",
    "tweets_long = tweets_long.dropna(subset=[\"created_at\"])\n",
    "tweets_long[\"date\"] = tweets_long[\"created_at\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab18dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_long[\"is_retweet\"] = np.where(\n",
    "    tweets_long[\"retweeted_status.id\"].notna(), 1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dacbbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    s = re.sub(r\"@\\w+\", \"\", s)\n",
    "    s = re.sub(r\"\\$\\w+\", \"\", s)\n",
    "    s = s.replace(\"#\", \"\")\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "tweets_long[\"clean_text\"] = tweets_long[\"base_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d350833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DIS</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>weekly dow stocks trend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WMT</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>weekly dow stocks trend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HD</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>weekly dow stocks trend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GS</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>weekly dow stocks trend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>weekly dow stocks trend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date               clean_text\n",
       "0    DIS  2014-01-03  weekly dow stocks trend\n",
       "0    WMT  2014-01-03  weekly dow stocks trend\n",
       "0     HD  2014-01-03  weekly dow stocks trend\n",
       "0     GS  2014-01-03  weekly dow stocks trend\n",
       "0      V  2014-01-03  weekly dow stocks trend"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_long[[\"ticker\", \"date\", \"clean_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1852e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_long.shape\n",
    "tweets_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40cdb5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398104, 9),\n",
       "   ticker        date                created_at               clean_text  \\\n",
       " 0    DIS  2014-01-03 2014-01-03 11:01:06+00:00  weekly dow stocks trend   \n",
       " 0    WMT  2014-01-03 2014-01-03 11:01:06+00:00  weekly dow stocks trend   \n",
       " 0     HD  2014-01-03 2014-01-03 11:01:06+00:00  weekly dow stocks trend   \n",
       " 0     GS  2014-01-03 2014-01-03 11:01:06+00:00  weekly dow stocks trend   \n",
       " 0      V  2014-01-03 2014-01-03 11:01:06+00:00  weekly dow stocks trend   \n",
       " \n",
       "                                            base_text  is_retweet  \\\n",
       " 0  Weekly Dow #Stocks Trend $DIS $WMT $HD $GS $V ...           0   \n",
       " 0  Weekly Dow #Stocks Trend $DIS $WMT $HD $GS $V ...           0   \n",
       " 0  Weekly Dow #Stocks Trend $DIS $WMT $HD $GS $V ...           0   \n",
       " 0  Weekly Dow #Stocks Trend $DIS $WMT $HD $GS $V ...           0   \n",
       " 0  Weekly Dow #Stocks Trend $DIS $WMT $HD $GS $V ...           0   \n",
       " \n",
       "    retweet_count  favorite_count  user.followers_count  \n",
       " 0              0               0                   546  \n",
       " 0              0               0                   546  \n",
       " 0              0               0                   546  \n",
       " 0              0               0                   546  \n",
       " 0              0               0                   546  )"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_cols = [\n",
    "    \"ticker\",\n",
    "    \"date\",\n",
    "    \"created_at\",\n",
    "    \"clean_text\",\n",
    "    \"base_text\",\n",
    "    \"is_retweet\",\n",
    "    \"retweet_count\",\n",
    "    \"favorite_count\",\n",
    "    \"user.followers_count\"\n",
    "]\n",
    "\n",
    "keep_cols = [c for c in keep_cols if c in tweets_long.columns]\n",
    "\n",
    "tweets_clean = tweets_long[keep_cols].copy()\n",
    "tweets_clean.shape, tweets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf4c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweets: 398104\n",
      "After basic noise filters: 249021\n",
      "High-impact tweets: 81803\n",
      "Retention rate: 0.2055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>created_at</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>base_text</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>followers</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>has_event_word</th>\n",
       "      <th>is_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VZ</td>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>2014-01-05 20:12:05+00:00</td>\n",
       "      <td>the s amp ps worst sectors in 2013</td>\n",
       "      <td>$VZ The S&amp;amp;Ps Worst Sectors in 2013 http://...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VZ</td>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>2014-01-15 09:28:46+00:00</td>\n",
       "      <td>ahah had to search ticker as well after i saw ...</td>\n",
       "      <td>@maoxian ahah had to search ticker as well aft...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6069</td>\n",
       "      <td>6069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T</td>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>2014-01-15 09:28:46+00:00</td>\n",
       "      <td>ahah had to search ticker as well after i saw ...</td>\n",
       "      <td>@maoxian ahah had to search ticker as well aft...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6069</td>\n",
       "      <td>6069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VZ</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>2014-01-21 17:04:22+00:00</td>\n",
       "      <td>sorry t mobile verizon is still the mightiest ...</td>\n",
       "      <td>$VZ - Sorry T-Mobile, Verizon Is Still the Mig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IBM</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>2014-01-21 15:24:20+00:00</td>\n",
       "      <td>hoy reportaran al cierre de mercado entre otra...</td>\n",
       "      <td>Hoy reportaran al cierre de mercado $IBM, $TXN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073</td>\n",
       "      <td>1073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker        date                created_at  \\\n",
       "2      VZ  2014-01-05 2014-01-05 20:12:05+00:00   \n",
       "11     VZ  2014-01-15 2014-01-15 09:28:46+00:00   \n",
       "11      T  2014-01-15 2014-01-15 09:28:46+00:00   \n",
       "17     VZ  2014-01-21 2014-01-21 17:04:22+00:00   \n",
       "20    IBM  2014-01-21 2014-01-21 15:24:20+00:00   \n",
       "\n",
       "                                           clean_text  \\\n",
       "2                  the s amp ps worst sectors in 2013   \n",
       "11  ahah had to search ticker as well after i saw ...   \n",
       "11  ahah had to search ticker as well after i saw ...   \n",
       "17  sorry t mobile verizon is still the mightiest ...   \n",
       "20  hoy reportaran al cierre de mercado entre otra...   \n",
       "\n",
       "                                            base_text  is_retweet  \\\n",
       "2   $VZ The S&amp;Ps Worst Sectors in 2013 http://...           0   \n",
       "11  @maoxian ahah had to search ticker as well aft...           0   \n",
       "11  @maoxian ahah had to search ticker as well aft...           0   \n",
       "17  $VZ - Sorry T-Mobile, Verizon Is Still the Mig...           0   \n",
       "20  Hoy reportaran al cierre de mercado $IBM, $TXN...           0   \n",
       "\n",
       "    retweet_count  favorite_count  user.followers_count  followers  retweets  \\\n",
       "2               0               0                     9          9         0   \n",
       "11              0               0                  6069       6069         0   \n",
       "11              0               0                  6069       6069         0   \n",
       "17              0               0                  1461       1461         0   \n",
       "20              0               0                  1073       1073         0   \n",
       "\n",
       "    likes  has_event_word  is_noise  \n",
       "2       0            True     False  \n",
       "11      0           False     False  \n",
       "11      0           False     False  \n",
       "17      0           False     False  \n",
       "20      0            True     False  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# start from the cleaned tweets dataframe\n",
    "df = tweets_clean.copy()\n",
    "\n",
    "# -------------------------\n",
    "# 1. ACCOUNT / ENGAGEMENT FEATURES\n",
    "# -------------------------\n",
    "df[\"followers\"] = df[\"user.followers_count\"].fillna(0)\n",
    "df[\"retweets\"]  = df[\"retweet_count\"].fillna(0)\n",
    "df[\"likes\"]     = df[\"favorite_count\"].fillna(0)\n",
    "\n",
    "# -------------------------\n",
    "# 2. EVENT / NEWS KEYWORDS\n",
    "# -------------------------\n",
    "EVENT_WORDS = [\n",
    "    \"earnings\", \"eps\", \"guidance\", \"forecast\",\n",
    "    \"revenue\", \"downgrade\", \"upgrade\",\n",
    "    \"merger\", \"acquisition\", \"acquire\",\n",
    "    \"lawsuit\", \"sec\", \"fined\", \"regulation\",\n",
    "    \"bankruptcy\", \"chapter 11\",\n",
    "    \"report\", \"announcement\",\n",
    "    \"dividend\", \"split\",\n",
    "    \"plunge\", \"surge\", \"breakout\", \"crash\"\n",
    "]\n",
    "event_regex = re.compile(\"|\".join(EVENT_WORDS), re.IGNORECASE)\n",
    "\n",
    "df[\"has_event_word\"] = df[\"clean_text\"].apply(\n",
    "    lambda x: bool(event_regex.search(x))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3. REMOVE NOISE / SPAM\n",
    "# -------------------------\n",
    "noise_regex = re.compile(r\"(penny|alert|follow|giveaway|\\$\\$\\$)\", re.IGNORECASE)\n",
    "df[\"is_noise\"] = df[\"clean_text\"].apply(lambda x: bool(noise_regex.search(x)))\n",
    "\n",
    "# keep only non-noise\n",
    "df = df[df[\"is_noise\"] == False]\n",
    "\n",
    "# drop very short tweets\n",
    "df = df[df[\"clean_text\"].str.len() >= 15]\n",
    "\n",
    "# drop retweets (keep originals only)\n",
    "df = df[df[\"is_retweet\"] == 0]\n",
    "\n",
    "# -------------------------\n",
    "# 4. NOW build the account mask (AFTER filtering)\n",
    "# -------------------------\n",
    "account_mask = (\n",
    "    (df[\"followers\"] >= 1000) |\n",
    "    (df[\"retweets\"] >= 50)   |\n",
    "    (df[\"likes\"]    >= 500)\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5. FINAL HIGH-IMPACT SELECTION\n",
    "# -------------------------\n",
    "high_impact = df[account_mask | df[\"has_event_word\"]].copy()\n",
    "\n",
    "print(\"Original tweets:\", len(tweets_clean))\n",
    "print(\"After basic noise filters:\", len(df))\n",
    "print(\"High-impact tweets:\", len(high_impact))\n",
    "print(\"Retention rate:\", round(len(high_impact) / len(tweets_clean), 4))\n",
    "\n",
    "high_impact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6893952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean.to_pickle(\"tweets_clean.pkl\")\n",
    "tweets_clean.to_csv(\"tweets_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3662a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_impact.to_pickle(\"tweets_high_impact.pkl\")\n",
    "high_impact.to_csv(\"tweets_high_impact.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
